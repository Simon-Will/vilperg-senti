\documentclass[a4paper]{article}

\usepackage{setspace}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{siunitx}

%\usepackage[
%	backend=biber,
%	citestyle=authoryear,
%	sortlocale=de_DE
%]{biblatex}
%\addbibresource{statusbericht.bib}

\onehalfspacing

\usepackage{booktabs}
\newcommand{\ourhighlight}[1]{\textit{#1}}
\newcommand{\ourfile}[1]{\texttt{#1}}
\newcommand{\ourextension}[1]{\texttt{#1}}

\title{Abschlussbericht: Sentimentanalyse auf Amazon-Reviews\\[0.5cm]
\large \textit{Designing Experiments for Machine Learning Tasks}\\[0.2cm]
\large bei Éva Mújdricza-Maydt\\[0.2cm]
\large Institut für Computerlinguistik\\[0.2cm]
\large Ruprecht-Karls-Universität Heidelberg\\}

\author{Caroline Berg \and Simon Will}
\date{\today}

\begin{document}
\maketitle
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Einführung}

\ourhighlight{Sentiment analysis} als Teilgebiet des maschinellen Lernens verfolgt die Aufgabe, einem gegebenen Text, oder Teilen des Textes, einen entsprechenden \ourhighlight{sentiment}-Wert, d.\,h. eine Zahl auf einer definierten Skala, die etwas über den Grad der Positivität, bzw. Negativität des Textes aussagt, zuzuordnen.\newline
Als Datengrundlage haben wir Review-Texte von Amazon gewählt. Diese schienen sich gut für eine Verarbeitung mithilfe des maschinellen Lernens zu eignen, da ein Kunde, der ein Review verfasst außerdem eine Anzahl von Sternen angeben muss, welche sich als Klassenattribut zum Trainieren eines Algorithmus eignet. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Ähnliche Projekte}

Obwohl wir uns nicht direkt an einem bereits vorhandenen Projekt aus diesem Bereich orientiert haben, kann man als vergleichbare Arbeit und zur kritischen Bewertung des Aufbaus unserer Sentiment-Analyse das Projekt \emph{Analyses in Amazon Reviews Using Probablistic Machine Learning} von Callen \textsc{Rain} heranziehen. 
Für diese Arbeit wurden gezielt Texte zu Produkten als Datengrundlage verwendet, die entweder besonders häufig bewertet wurden (z.\,B. Bücher, CDs, Filme, AmazonKindle) oder kaum Reviews aufwiesen (z.\,B. LeviJeans, MacBook).
Das Problem, dass Reviews mit vielen Sternen häufiger sind als solche mit wenigen, wurde von \textsc{Rain} gelöst, indem er nur Reviews mit 5 Sternen (\emph{score} 1) und Reviews mit 1--2 Sternen (\emph{score} 0) berücksichtigt hat.
Alle dazwischenliegenden Bewertungen wurden nicht in die Datengrundlage aufgenommen.

Zu erwähnende Attribute in diesem Projekt sind zum einen der \emph{bag-of-words}-Ansatz bezüglich der 2000 häufigsten Wörter und der 500 häufigsten Bigramme, die Beachtung von Negation, sowie die Satzlänge der Reviewtexte, wobei besonders lange und besonders kurze Sätze in die Analyse miteinbezogen wurden. Die besten Ergebnisse wurden mit dem \emph{Naïve Bayes Classifier} auf Reviews von AmazonKindle (\SI{84}{\%} \emph{accuracy}) und dem MacBook (\SI{88.2}{\%} \emph{accuracy}) erzielt. Zu beachten ist hierbei, dass die \emph{baseline} bei \SI{50}{\%} liegt, da lediglich binär klassifiziert wurde.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Vorverarbeitung der Daten}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Amazon-Scraper}

Der Amazon-Scraper dient dem Herunterladen der für das Projekt erforderlichen Daten (z.\,B. bewertetes Produkt, Reviewtext und Anzahl der vergebenen Sterne). Hierbei wird automatisch die jeweils folgende Seite auf Amazon aufgerufen (es werden immer 10 Reviews pro Seite angezeigt) und die gesammelte Datenmenge wird im angegebenen Verzeichnis abgespeichert.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{TreeTagger}

Um die Daten zu formalisieren haben wir uns für den TreeTagger entschieden, welcher die Texte tokenisiert, lemmatisiert und mit POS-Tags versieht. Es wurden überraschend gute Resultate erzielt, beispielsweise wurden klein geschriebene Nomen richtig erkannt.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Chunks}

Mit Blick auf die Verteilung der Sterne auf den heruntergeladenen Daten, fällt sofort auf, dass Produkte meist sehr gute Bewertungen (4--5 Sterne), eher selten sehr schlechte (1 Stern) und kaum mangelhafte Bewertungen (2--3 Sterne) verliehen bekommen.

Um zu gewährleisten, dass die Algorithmen jeweils auf der gleichen Menge Daten pro Anzahl von Sternen trainieren können, müssen die Daten balanciert werden. 
Dafür haben wir zunächst randomisierte symbolische Links erstellt, die auf ein Review zeigen, und die Links anschließend in balancierten Chunks gespeichert. 
Durch das Verfahren, welches uns eine ausgewogene Datenmenge zum trainieren bereitstellt, mussten wir allerdings im Schnitt den Verlust von etwa einem Viertel unserer anfangs heruntergeladenen Daten in Kauf nehmen.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{SentiWS und Attribute}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Sentimentannotation}

Als deutschsprachige Datenbank, um das Sentiment eines Reviews zu bestimmen, bot sich SentiWS an. Es umfasst ungefähr 33000 Wortformen und 3500 Lemmata.
Dabei wird jedem Wort ein Wert auf einer Skala von $-1$ bis $1$ zugeordnet, wobei ein positiver Wert eine positive Konnotation anzeigt, ein negativer eine negative.
In unserem Fall wird für einen Reviewtext geprüft, welche Wortformen ebenfalls in SentiWS vorkommen. Die aufsummierten Werte werden dann für die jeweilige Wortform (s. \ref{features}) separat gespeichert.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Attribute}
\label{features}

Zur Extraktion der Features dient ein Python-Programm, das alternativ auch mit einem Shell-Skript aufgerufen werden kann.\newline 
Das Attribut \texttt{token\_number} gibt die Länge des Reviewtextes an. Desweiteren werden die Sentimentwerte zum einen in Wortklassen eingeteilt. Wir haben jeweils ein Attribut für \texttt{adjective\_sentiment}, \texttt{noun\_sentiment} und \texttt{verb\_sentiment} erstellt. Außerdem enthält das Attribut \texttt{overall\_sentiment} die Summe aller Sentimentwerte, die gefunden wurden.\newline
Alle Sentiment-Attribute können der \ourextension{arff}-Datei auch in einer normierten Version übergeben werden, die mit der folgenden Formel berechnet wird:
$$\text{normalized\_sentiment} = \frac{\text{sentiment} \times 10^6}{\text{token\_number}}$$
Als Klassenattribut kann entweder die Anzahl der Sterne für das jeweilige Review oder in einer weiteren Version das Attribut \texttt{binary\_judgement,} welches allen Reviews mit 1--2 Sternen den Wert 0 und allen Reviews mit 4--5 Sternen den Wert 1 zuteilt, verwendet werden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimente und Ergebnisse}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Klassifizierer}

Die besten Resultate haben wir mit folgenden Klassifizierern erzielt:
\begin{itemize}
	\item J48 
	\item RandomForest
	\item Naïve Bayes
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Erzielte Resultate}

\begin{table}[!h]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        Klassen & normiert & Majority Voting & J48 & RandomForest & Naive Bayes \\
        \midrule
        1--5 & nein & \SI{20}{\%} &\SI{61.0}{\%} & \SI{67.4}{\%} & \SI{31.6}{\%} \\
        1--5 & ja & \SI{20}{\%} &\SI{61.0}{\%} & \SI{67.6}{\%} & \SI{32.5}{\%} \\
        binär & ja & \SI{60}{\%} & \SI{75.0}{\%} & \SI{84.6}{\%} & \SI{69.8}{\%} \\
        \bottomrule
    \end{tabular}
    \caption{Resultate mit zehnfacher Kreuzvalidierung}
    \label{results}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Test auf fremder Domäne}

Für das Testen auf einer fremden Domäne haben wir zunächst auf Smartphone-Reviews trainiert, wobei die \ourextension{arff}-Datei mit normierten Sentimentattributen  erstellt wurde. Hierfür haben wir ein ernüchterndes Ergebnis von \SI{27.7}{\%} erzielt, was wir darauf zurückführen, dass Armbanduhren-Reviews im Schnitt nur circa ein Drittel der Länge von Smartphone-Reviews aufweisen.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Fazit}


Trotz unseres naiven Ansatzes, was die Attributwahl betrifft, haben wir überraschend gute Ergebnisse erzielt.\newline
Allgemein sollte man bei der Bewertung der Ergebnisse beachten, dass es zwischen den Reviewtexten erhebliche Unterschiede bezüglich der Formulierung, dem Umfang und der Komplexität des Textinhaltes gibt, da den Verfassern keine Normen oder Formalia bezüglich des Inhaltes oder Schreibstils vorgeschrieben werden. Durch diese Variation in Qualität und Quantität der Texte ergeben sich daher Probleme bei der Klassifizierung mittels Lernalgorithmen. 

Um die Sentimentanalyse zu verbessern, müsste man die Attributwahl etwas komplexer gestalten. Negierende Ausdrücke sollten vermerkt und gegebenenfalls aufgelöst werden.
Ein weiteres Problem, das beim Sichten der Texte deutlich wird, ist, dass viele Nutzer in ihren Reviewtexten nicht über das eigentliche Produkt schreiben, sondern beispielsweise von ihren Erfahrungen mit ähnlichen Produkten oder den Lieferumständen berichten.

Um diesem Umstand gerecht zu werden, wäre es nötig, ein Topik-Attribut einführen, das beispielsweise nur Textabschnitte wertet, welche auch tatsächlich auf das zu bewertende Produkt referieren. Auch die Struktur des Textes könnte für das Sentiment eines Textes ausschlaggebend sein.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Literaturverzeichnis}

\textsc{Rain,} Callen: \textit{Sentiment Analysis in Amazon Reviews Using Probabilistic Machine Learning.} Swarthmore : Department of Computer Science, Swarthmore College, 2013.\\[0.1cm]
\textsc{Remus,} R.; \textsc{Quasthoff,} U.; \textsc{Heyer,} G.: \textit{SentiWS -- a Publicly Available German-language Resource for Sentiment Analysis.} In: \textit{Proceedings of the 7th International Language Ressources and Evaluation (LREC'10),} Valetta : 2010, S. 1168--1171.\\[0.1cm]
\textsc{Schmid,} Helmut.: \textit{Probabilistic Part-of-Speech Tagging Using Decision Trees.} In: \textit{Proceedings of International Conference on New Methods in Language Processing,} Manchester : 1994.
        %\printbibliography

\end{document}
